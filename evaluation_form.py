#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (CNN Feature + RandomForest) ì„±ëŠ¥ ë¹„êµ íŒŒì´í”„ë¼ì¸
=========================================================

í•µì‹¬ ì•„ì´ë””ì–´:
1. (ìœ ì§€) E2E ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ 2D ê³µê°„ íŒ¨í„´ì„ í•™ìŠµ (best_model.pth)
2. (ë³€ê²½) E2E ëª¨ë¸ì„ 'í”¼ì²˜ ì¶”ì¶œê¸°'ë¡œë§Œ ì‚¬ìš© (ë§ˆì§€ë§‰ head ë ˆì´ì–´ ì œê±°)
3. (ì¶”ê°€) ğŸ”¥ RandomForest ëª¨ë¸ì„ 2ê°€ì§€ ë²„ì „ìœ¼ë¡œ í•™ìŠµ ë° ë¹„êµ:
    - A: RandomForest + ê¸°ë³¸ í”¼ì²˜ (ìƒ˜í”Œ ì½”ë“œ ë°©ì‹)
    - B: RandomForest + ê¸°ë³¸ í”¼ì²˜ + CNN í”¼ì²˜ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)
4. (ì¶”ê°€) ğŸ”¥ Validation Set (NG=15, Good=45)ì—ì„œ 'ì‚¬ì§„ ì† ìµœì¢… ê³µì‹'ìœ¼ë¡œ ì ìˆ˜ ë¹„êµ
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, SequentialSampler
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split 
from sklearn.ensemble import RandomForestClassifier # ğŸ”¥ ìƒ˜í”Œ ì½”ë“œì™€ ë™ì¼í•œ ëª¨ë¸
from sklearn.metrics import roc_auc_score
# ğŸ”¥ ì¶”ê°€ ëª¨ë¸ë“¤
try:
    from catboost import CatBoostClassifier
    CATBOOST_AVAILABLE = True
    print("âœ… CatBoost ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    CATBOOST_AVAILABLE = False
    print("âš ï¸ CatBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. conda install -c conda-forge catboost")
import warnings
import matplotlib.pyplot as plt
import platform

# 1. í•œê¸€ í°íŠ¸ ì„¤ì •
if platform.system() == 'Windows':
    plt.rcParams['font.family'] = 'Malgun Gothic'
elif platform.system() == 'Darwin':
    plt.rcParams['font.family'] = 'AppleGothic'
elif platform.system() == 'Linux':
    plt.rcParams['font.family'] = 'NanumGothic'

# 2. ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

# ----------------------------------------------------
# 1. ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤ (ë³€ê²½ ì—†ìŒ)
# ----------------------------------------------------
class DataProcessor:
    """ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.OE = None
        self.Scaler = None
        self.cat_list = None
        self.num_list = None
        self.x_min_global = None
        self.x_max_global = None
        self.y_min_global = None
        self.y_max_global = None
        self.basic_feature_dim = None 
    
    def load_data(self, train_path="train.csv", test_path="test.csv"): 
        """ë°ì´í„° ë¡œë”©"""
        self.train = pd.read_csv(train_path)
        # test.csvëŠ” ì¢Œí‘œ ë²”ìœ„ ë¶„ì„(analyze_coordinate_range)ì„ ìœ„í•´ì„œë§Œ ë¡œë“œ
        self.test = pd.read_csv(test_path) 
        
        print(f"Train shape: {self.train.shape}")
        print(f"Test shape: {self.test.shape}") 
        
        self.train_X_basic = self.train.drop(columns=['Class']).iloc[:,:-256*3]
        self.train_Y = self.train['Class'].apply(lambda x: 1 if x == 'NG' else 0) # NG=1
        
        print(f"Features shape (Train): {self.train_X_basic.shape}")
        print(f"Target distribution - Good: {(self.train_Y==0).sum()}, NG: {(self.train_Y==1).sum()}")
        
        return self.train, self.test, self.train_X_basic, self.train_Y
    
    def setup_basic_preprocessing(self, train_X_basic_df):
        """ê¸°ë³¸ í”¼ì²˜ ì „ì²˜ë¦¬ ì„¤ì • (ğŸ”¥ ë¶„í• ëœ train setìœ¼ë¡œ fit)"""
        self.cat_list = train_X_basic_df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()
        self.num_list = sorted(list(set(train_X_basic_df.columns) - set(self.cat_list)))
        
        self.OE = OneHotEncoder(min_frequency=0.01, handle_unknown='infrequent_if_exist', sparse_output=False)
        self.OE.fit(train_X_basic_df[self.cat_list])
        
        self.Scaler = StandardScaler()
        self.Scaler.fit(train_X_basic_df[self.num_list])
        
    def preprocess_basic(self, dataset):
        """ê¸°ë³¸ í”¼ì²˜ ì „ì²˜ë¦¬"""
        Xc = self.OE.transform(dataset[self.cat_list])
        Xn = self.Scaler.transform(dataset[self.num_list])
        combined = np.concatenate([Xc, Xn], axis=1)
        
        if self.basic_feature_dim is None:
            self.basic_feature_dim = combined.shape[1]
            print(f"ê¸°ë³¸ í”¼ì²˜ ì°¨ì›: {self.basic_feature_dim}")
            
        return combined.astype(np.float32)
    
    def analyze_coordinate_range(self):
        """ì‹¤ì œ x, y ì¢Œí‘œ ë²”ìœ„ ë¶„ì„"""
        x_cols = [f'x{i}' for i in range(256)]
        y_cols = [f'y{i}' for i in range(256)]
        
        all_data = pd.concat([self.train, self.test], ignore_index=True) 
        x_values = all_data[x_cols].values.flatten()
        y_values = all_data[y_cols].values.flatten()
        
        x_values = x_values[~np.isnan(x_values)]
        y_values = y_values[~np.isnan(y_values)]
        
        self.x_min_global = x_values.min()
        self.x_max_global = x_values.max()
        self.y_min_global = y_values.min()
        self.y_max_global = y_values.max()
        
        print(f"ğŸ“Š ì¢Œí‘œ ë²”ìœ„ ë¶„ì„ ê²°ê³¼:")
        print(f"   X ì¢Œí‘œ ë²”ìœ„: {self.x_min_global:.2f} ~ {self.x_max_global:.2f}")
        print(f"   Y ì¢Œí‘œ ë²”ìœ„: {self.y_min_global:.2f} ~ {self.y_max_global:.2f}")
        
        return self.x_min_global, self.x_max_global, self.y_min_global, self.y_max_global

# ----------------------------------------------------
# 2. ë˜ìŠ¤í„°í™” í´ë˜ìŠ¤ (ë³€ê²½ ì—†ìŒ)
# ----------------------------------------------------
class SpatialRasterizer:
    def __init__(self, x_min, x_max, y_min, y_max, grid_size=64):
        self.x_min = x_min
        self.x_max = x_max
        self.y_min = y_min
        self.y_max = y_max
        self.grid_size = grid_size
        self.x_range = x_max - x_min if x_max > x_min else 1
        self.y_range = y_max - y_min if y_max > y_min else 1
        
    def rasterize_with_real_coordinates(self, data_row):
        x_cols = [f'x{i}' for i in range(256)]
        y_cols = [f'y{i}' for i in range(256)]
        p_cols = [f'p{i}' for i in range(256)]
        
        x_coords = data_row[x_cols].values
        y_coords = data_row[y_cols].values
        p_values = data_row[p_cols].values
        
        grid = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)
        count_grid = np.zeros((self.grid_size, self.grid_size), dtype=np.int8)

        for i in range(256):
            if not (np.isnan(x_coords[i]) or np.isnan(y_coords[i]) or np.isnan(p_values[i])):
                x_norm = (x_coords[i] - self.x_min) / self.x_range
                y_norm = (y_coords[i] - self.y_min) / self.y_range
                x_idx = int(np.clip(x_norm * (self.grid_size - 1), 0, self.grid_size - 1))
                y_idx = int(np.clip(y_norm * (self.grid_size - 1), 0, self.grid_size - 1))
                grid[y_idx, x_idx] += p_values[i]
                count_grid[y_idx, x_idx] += 1
        
        mask = count_grid > 0
        grid[mask] = grid[mask] / count_grid[mask]
        
        return grid
    
# ----------------------------------------------------
# 3. E2E ëª¨ë¸ ì •ì˜ (ë³€ê²½ ì—†ìŒ)
# ----------------------------------------------------

class ImageCNN(nn.Module):
    def __init__(self, output_dim=64, input_size=64):  # ğŸ”¥ output_dim 64â†’128, input_size 64â†’128
        super(ImageCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)      # ğŸ”¥ kernel_size 3â†’5
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) 
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)
        
        self.batch_norm1 = nn.BatchNorm2d(32)
        self.batch_norm2 = nn.BatchNorm2d(64)
        self.batch_norm3 = nn.BatchNorm2d(128)
        self.batch_norm4 = nn.BatchNorm2d(256)
        
        final_size = input_size // 16
        self.fc1 = nn.Linear(256 * final_size * final_size, 512)
        self.fc_out = nn.Linear(512, output_dim) 
        
        self.dropout = nn.Dropout(0.3)
    
    def forward(self, x):
        x = F.max_pool2d(F.relu(self.batch_norm1(self.conv1(x))), 2)
        x = F.max_pool2d(F.relu(self.batch_norm2(self.conv2(x))), 2)
        x = F.max_pool2d(F.relu(self.batch_norm3(self.conv3(x))), 2)
        x = F.max_pool2d(F.relu(self.batch_norm4(self.conv4(x))), 2)
        
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        return self.fc_out(x)


class FullE2EModel(nn.Module):
    def __init__(self, basic_feature_dim, image_cnn_output_dim=64, basic_mlp_output_dim=32, input_grid_size=64):  # ğŸ”¥ ê¸°ë³¸ê°’ ë³€ê²½
        super(FullE2EModel, self).__init__()
        
        self.image_cnn = ImageCNN(output_dim=image_cnn_output_dim, input_size=input_grid_size)
        
        self.basic_mlp = nn.Sequential(
            nn.Linear(basic_feature_dim, basic_feature_dim * 2),
            nn.ReLU(),
            nn.BatchNorm1d(basic_feature_dim * 2),
            nn.Dropout(0.3),
            nn.Linear(basic_feature_dim * 2, basic_mlp_output_dim),
            nn.ReLU()
        )
        
        combined_dim = image_cnn_output_dim + basic_mlp_output_dim  # 128 + 32 = 160
        self.head = nn.Sequential(
            nn.Linear(combined_dim, 64),  # ğŸ”¥ 64â†’128ë¡œ ì¦ê°€
            nn.ReLU(),
            nn.BatchNorm1d(64),           # ğŸ”¥ 64â†’128ë¡œ ì¦ê°€
            nn.Dropout(0.3),
            nn.Linear(64, 1)              # ğŸ”¥ 64â†’128ë¡œ ì¦ê°€
        )
    
    def forward(self, x_image, x_basic):
        img_feat = self.image_cnn(x_image)
        basic_feat = self.basic_mlp(x_basic)
        combined = torch.cat((img_feat, basic_feat), dim=1)
        output = self.head(combined) # 160ì°¨ì› -> 1ì°¨ì›
        return output

    # ğŸ”¥ ì¶”ê°€: í”¼ì²˜ ì¶”ì¶œì„ ìœ„í•œ 'ë¨¸ë¦¬ ì—†ëŠ”' forward
    def extract_features(self, x_image, x_basic):
        img_feat = self.image_cnn(x_image)
        basic_feat = self.basic_mlp(x_basic)
        combined = torch.cat((img_feat, basic_feat), dim=1)
        return combined # 160ì°¨ì› í”¼ì²˜ ë°˜í™˜


# ----------------------------------------------------
# 4. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ë³€ê²½ ì—†ìŒ)
# ----------------------------------------------------
class MultiModalDataset(Dataset):
    def __init__(self, full_df, basic_features_np, rasterizer, labels_np=None):
        self.full_df = full_df.reset_index(drop=True)
        self.basic_features_np = basic_features_np
        self.rasterizer = rasterizer
        self.labels_np = labels_np
        self.is_test = (labels_np is None)

    def __len__(self):
        return len(self.full_df)

    def __getitem__(self, idx):
        data_row = self.full_df.iloc[idx]
        image_grid = self.rasterizer.rasterize_with_real_coordinates(data_row)
        image_tensor = torch.from_numpy(image_grid).unsqueeze(0) # (1, 64, 64)
        basic_feat_tensor = torch.from_numpy(self.basic_features_np[idx])
        
        if self.is_test:
            # (ì´ ì½”ë“œëŠ” test ì˜ˆì¸¡ì„ ì•ˆí•˜ë¯€ë¡œ ì´ ë¶€ë¶„ì€ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            return image_tensor, basic_feat_tensor
        else:
            label_tensor = torch.tensor(self.labels_np[idx], dtype=torch.float32).view(1)
            return image_tensor, basic_feat_tensor, label_tensor

# ----------------------------------------------------
# 5. ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ (15:45 ìƒ˜í”Œë§, ìµœì¢… ê³µì‹ ì ìš©)
# ----------------------------------------------------
class HybridModelPipeline:
    """í•˜ì´ë¸Œë¦¬ë“œ (CNN + RandomForest) ë¹„êµ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, n_epochs=20, batch_size=32):
        self.data_processor = DataProcessor()
        self.rasterizer = None
        self.cnn_model = None # E2E ëª¨ë¸
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        self.n_epochs = n_epochs
        self.batch_size = batch_size
    
    def train_cnn_extractor(self, train_loader, val_loader):
        """E2E ëª¨ë¸ì„ 'í•™ìŠµ'ì‹œì¼œ í”¼ì²˜ ì¶”ì¶œê¸°(best_model.pth)ë¥¼ ë§Œë“­ë‹ˆë‹¤."""
        
        self.cnn_model = FullE2EModel(self.data_processor.basic_feature_dim, input_grid_size=64).to(self.device)
        optimizer = optim.Adam(self.cnn_model.parameters(), lr=1e-3, weight_decay=1e-5)
        criterion = nn.BCEWithLogitsLoss() 
        
        best_val_loss = np.inf
        print("\nğŸ§  1ë‹¨ê³„: CNN í”¼ì²˜ ì¶”ì¶œê¸° í•™ìŠµ ì‹œì‘...")
        
        for epoch in range(self.n_epochs):
            self.cnn_model.train()
            train_loss_total = 0.0
            
            for img, basic, labels in train_loader:
                img, basic, labels = img.to(self.device), basic.to(self.device), labels.to(self.device)
                optimizer.zero_grad()
                outputs = self.cnn_model(img, basic) # E2E ëª¨ë¸ í•™ìŠµ
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                train_loss_total += loss.item()
            
            # Validation
            self.cnn_model.eval()
            val_loss_total = 0.0
            with torch.no_grad():
                for img, basic, labels in val_loader:
                    img, basic, labels = img.to(self.device), basic.to(self.device), labels.to(self.device)
                    outputs = self.cnn_model(img, basic)
                    loss = criterion(outputs, labels)
                    val_loss_total += loss.item()
            
            avg_train_loss = train_loss_total / len(train_loader)
            avg_val_loss = val_loss_total / len(val_loader)
            
            print(f"  Epoch [{epoch+1}/{self.n_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
            
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save(self.cnn_model.state_dict(), 'best_model.pth')
                print(f"     -> Best CNN Extractor saved with Val Loss: {best_val_loss:.4f}")
        print("âœ… CNN í”¼ì²˜ ì¶”ì¶œê¸° í•™ìŠµ ì™„ë£Œ.")

    def extract_cnn_features(self, loader):
        """'í•™ìŠµëœ' E2E ëª¨ë¸ì„ ì‚¬ìš©í•´ 'CNN í”¼ì²˜'ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        
        # ì €ì¥ëœ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
        if self.cnn_model is None:
            self.cnn_model = FullE2EModel(self.data_processor.basic_feature_dim, input_grid_size=64).to(self.device)
        self.cnn_model.load_state_dict(torch.load('best_model.pth'))
        self.cnn_model.eval()
        
        all_features = []
        with torch.no_grad():
            for img, basic, labels in loader: # ë ˆì´ë¸”ì€ ì‚¬ìš© ì•ˆí•¨
                img, basic = img.to(self.device), basic.to(self.device)
                
                # 'head'ë¥¼ ì œê±°í•˜ê³  96ì°¨ì› í”¼ì²˜ ì¶”ì¶œ
                features_batch = self.cnn_model.extract_features(img, basic)
                all_features.append(features_batch.cpu().numpy())
        
        return np.concatenate(all_features, axis=0)

    def calculate_competition_score(self, y_true, y_prob):
        """
        ğŸ”¥ (ìµœì¢… ìˆ˜ì •) Validation Setì—ì„œ 'ì‚¬ì§„ ì† ìµœì¢… ê³µì‹'ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚°
        - y_true: 0(Good), 1(NG) (ì´ 60ê°œ)
        - y_prob: NG(ë¶ˆëŸ‰)ì¼ í™•ë¥  (0~1)
        """
        
        # 1. TASK 1: ROC-AUC Score
        roc_auc = roc_auc_score(y_true, y_prob)
        
        # 2. TASK 2: Total Net Profit
        # k: 'decision=True'ë¡œ ì„ íƒí•  ê°œìˆ˜ (15ê°œ)
        k = 15 # ğŸ”¥ 9 -> 15ë¡œ ìˆ˜ì •
            
        df_eval = pd.DataFrame({'prob': y_prob, 'true_label': y_true})
        
        # 'decision=True'ì¸ Top kê°œ (ë¶ˆëŸ‰ë¥ ì´ ê°€ì¥ ë‚®ì€ kê°œ) ì„ íƒ
        selected_products_df = df_eval.nsmallest(k, 'prob')
        
        # ë§ì¶˜ Good(0) ê°œìˆ˜
        correct_good_count = (selected_products_df['true_label'] == 0).sum()
        # í‹€ë¦° NG(1) ê°œìˆ˜
        incorrect_ng_count = (selected_products_df['true_label'] == 1).sum()

        # Net Profit ê³„ì‚°
        total_net_profit = (100 * correct_good_count) - (150 * incorrect_ng_count)

        # 3. Final Total Score
        auc_comp = max(roc_auc - 0.5, 0) / 0.5
        profit_comp = max(total_net_profit, 0) / 1500  # 20000ì€ ê³ ì • ìŠ¤ì¼€ì¼ë§ ê°’
        
        total_score = np.sqrt(auc_comp * profit_comp)
        
        return total_score, roc_auc, total_net_profit, k


    def run_comparison_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=" * 60)
        print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ (CNN+RF) vs ê¸°ë³¸ (RF) ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
        print("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë”©
        print("\nğŸ“ 1ë‹¨ê³„: ë°ì´í„° ë¡œë”© (train.csv, test.csv)")
        train_df, test_df, train_X_basic_df, train_Y_series = \
            self.data_processor.load_data(train_path="train.csv", test_path="test.csv")
        
        # 2. ì¢Œí‘œ ë²”ìœ„ ë¶„ì„
        print("\nğŸ“Š 2ë‹¨ê³„: ì¢Œí‘œ ë²”ìœ„ ë¶„ì„ (Train+Test í†µí•©)")
        x_min, x_max, y_min, y_max = self.data_processor.analyze_coordinate_range()
        
        # 3. ë˜ìŠ¤í„°í™” ì„¤ì •
        print("\nğŸ¯ 3ë‹¨ê³„: ê³µê°„ ë˜ìŠ¤í„°í™” ì„¤ì •")
        self.rasterizer = SpatialRasterizer(x_min, x_max, y_min, y_max, grid_size=64)
        
        # 4. ğŸ”¥ Train / Validation ë°ì´í„° ë¶„ë¦¬ (NG=15, Good=45)
        print("\nğŸ”ª 4ë‹¨ê³„: Train / Validation ë°ì´í„° ë¶„ë¦¬ (NG=15, Good=45)")
        
        all_indices = train_Y_series.index
        all_labels = train_Y_series.values
        
        ng_indices = all_indices[all_labels == 1]
        good_indices = all_indices[all_labels == 0]
        
        # ğŸ”¥ 9 -> 15ë¡œ ìˆ˜ì •
        val_ng_count = min(15, len(ng_indices))
        # ğŸ”¥ 27 -> 45ë¡œ ìˆ˜ì •
        val_good_count = min(45, len(good_indices))
        
        if len(ng_indices) < 15 or len(good_indices) < 45:
            print(f"âš ï¸ ê²½ê³ : ë°ì´í„° ë¶€ì¡±. NG {len(ng_indices)}ê°œ, Good {len(good_indices)}ê°œ.")
            print(f"   -> Val Setì„ NG={val_ng_count}ê°œ, Good={val_good_count}ê°œë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.")

        np.random.seed(42) # ì¬í˜„ì„±ì„ ìœ„í•´
        val_ng_indices = np.random.choice(ng_indices, val_ng_count, replace=False)
        val_good_indices = np.random.choice(good_indices, val_good_count, replace=False)
        
        val_indices = np.concatenate([val_ng_indices, val_good_indices])
        
        # train_indicesëŠ” val_indicesë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€
        train_indices = np.setdiff1d(all_indices, val_indices)
        
        
        # ê¸°ë³¸ í”¼ì²˜ (Pandas)
        train_X_basic_split_df = train_X_basic_df.iloc[train_indices]
        val_X_basic_split_df = train_X_basic_df.iloc[val_indices]
        
        # ì „ì²´ ë°ì´í„° (Pandas)
        train_df_split = train_df.iloc[train_indices]
        val_df_split = train_df.iloc[val_indices]

        # ë ˆì´ë¸” (Pandas Series)
        y_train_labels = train_Y_series.iloc[train_indices]
        y_val_labels = train_Y_series.iloc[val_indices]
        
        print(f"  Train set: {len(train_indices)}ê°œ, Validation set: {len(val_indices)}ê°œ")
        print(f"  (Val Set êµ¬ì„±: NG={val_ng_count}ê°œ, Good={val_good_count}ê°œ)")

        # 5. ê¸°ë³¸ í”¼ì²˜ ì „ì²˜ë¦¬ (Numpy)
        print("\nğŸ”„ 5ë‹¨ê³„: ê¸°ë³¸ í”¼ì²˜ ì „ì²˜ë¦¬ (Numpy ë³€í™˜)")
        # ğŸ”¥ ì¤‘ìš”: .setup_basic_preprocessingì„ train_setìœ¼ë¡œë§Œ fit
        self.data_processor.setup_basic_preprocessing(train_X_basic_split_df) 
        
        X_train_basic_np = self.data_processor.preprocess_basic(train_X_basic_split_df)
        X_val_basic_np = self.data_processor.preprocess_basic(val_X_basic_split_df)
        
        # 6. ğŸ”¥ CNN í•™ìŠµìš© ë°ì´í„°ì…‹/ë¡œë” ìƒì„±
        print("\nğŸ“¦ 6ë‹¨ê³„: CNN í•™ìŠµìš© ë°ì´í„°ì…‹/ë¡œë” ìƒì„±")
        train_dataset = MultiModalDataset(train_df_split, X_train_basic_np, self.rasterizer, y_train_labels.values)
        val_dataset = MultiModalDataset(val_df_split, X_val_basic_np, self.rasterizer, y_val_labels.values)
        
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
        
        # 7. ğŸ”¥ CNN í”¼ì²˜ ì¶”ì¶œê¸° í•™ìŠµ
        self.train_cnn_extractor(train_loader, val_loader)
        
        # 8. ğŸ”¥ CNN í”¼ì²˜ ì¶”ì¶œ (ìˆœì„œê°€ ì¤‘ìš”í•˜ë¯€ë¡œ Shuffle=False)
        print("\nâœ¨ 7ë‹¨ê³„: CNN í”¼ì²˜ ì¶”ì¶œ (Train/Val Set)")
        train_loader_seq = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=False)
        val_loader_seq = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
        
        X_train_cnn_feats = self.extract_cnn_features(train_loader_seq)
        X_val_cnn_feats = self.extract_cnn_features(val_loader_seq)
        print(f"  ì¶”ì¶œëœ CNN í”¼ì²˜ í˜•íƒœ (Train): {X_train_cnn_feats.shape}") 
        print(f"  ì¶”ì¶œëœ CNN í”¼ì²˜ í˜•íƒœ (Val): {X_val_cnn_feats.shape}")   
        
        # 9. ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ í”¼ì²˜ ìƒì„±
        print("\nğŸ§¬ 8ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ í”¼ì²˜ ê²°í•© (ê¸°ë³¸ + CNN)")
        X_train_hybrid = np.concatenate([X_train_basic_np, X_train_cnn_feats], axis=1)
        X_val_hybrid = np.concatenate([X_val_basic_np, X_val_cnn_feats], axis=1)
        print(f"  í•˜ì´ë¸Œë¦¬ë“œ í”¼ì²˜ í˜•íƒœ (Train): {X_train_hybrid.shape}") 
        
        # 10. ğŸ”¥ ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ë¹„êµ
        print("\nğŸ¤– 9ë‹¨ê³„: ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ë¹„êµ")
        
        models_results = {}
        
        # ëª¨ë¸ 1: RandomForest (ê¸°ë³¸)
        print("\nğŸŒ² RandomForest ëª¨ë¸ í•™ìŠµ...")
        rf_model = RandomForestClassifier(
            random_state=42, 
            n_estimators=300,        # ğŸ”¥ ë” ë§ì€ íŠ¸ë¦¬
            max_depth=10,           # ğŸ”¥ ê¹Šì´ ì œí•œ
            min_samples_split=5,    # ğŸ”¥ ë¶„í•  ìµœì†Œ ìƒ˜í”Œ
            min_samples_leaf=2,     # ğŸ”¥ ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ
            max_features='sqrt',    # ğŸ”¥ í”¼ì²˜ ì„ íƒ ë°©ì‹
            bootstrap=True,         # ğŸ”¥ ë¶€íŠ¸ìŠ¤íŠ¸ë©
            n_jobs=-1
        )
        rf_model.fit(X_train_hybrid, y_train_labels)
        rf_prob = rf_model.predict_proba(X_val_hybrid)[:, 1]
        rf_score, rf_auc, rf_profit, rf_k = self.calculate_competition_score(y_val_labels.values, rf_prob)
        models_results['RandomForest'] = {
            'model': rf_model, 'prob': rf_prob, 'score': rf_score, 
            'auc': rf_auc, 'profit': rf_profit, 'k': rf_k
        }
        
        # ëª¨ë¸ 2: CatBoost (ì¶”ì²œ)
        if CATBOOST_AVAILABLE:
            print("\nğŸ± CatBoost ëª¨ë¸ í•™ìŠµ...")
            cat_model = CatBoostClassifier(
                random_seed=42, 
                iterations=300,          # ğŸ”¥ ë” ë§ì€ ë°˜ë³µ
                depth=6,
                learning_rate=0.1,
                l2_leaf_reg=3,          # ğŸ”¥ ì •ê·œí™” ì¶”ê°€
                bootstrap_type='Bernoulli',  # ğŸ”¥ ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°©ì‹
                subsample=0.8,          # ğŸ”¥ ìƒ˜í”Œë§ ë¹„ìœ¨
                verbose=False,
                eval_metric='AUC',      # ğŸ”¥ AUC ìµœì í™”
                early_stopping_rounds=50  # ğŸ”¥ ì¡°ê¸° ì¢…ë£Œ
            )
            cat_model.fit(X_train_hybrid, y_train_labels)
            cat_prob = cat_model.predict_proba(X_val_hybrid)[:, 1]
            cat_score, cat_auc, cat_profit, cat_k = self.calculate_competition_score(y_val_labels.values, cat_prob)
            models_results['CatBoost'] = {
                'model': cat_model, 'prob': cat_prob, 'score': cat_score,
                'auc': cat_auc, 'profit': cat_profit, 'k': cat_k
            }
        else:
            print("âš ï¸ CatBoostë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. RandomForestë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        best_model_name = max(models_results.keys(), key=lambda k: models_results[k]['score'])
        best_result = models_results[best_model_name]

        # 11. ğŸ”¥ ìµœì¢… ê²°ê³¼ (ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ)
        print("\n" + "=" * 80)
        print("ğŸ‰ ë‹¤ì¤‘ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ (í•˜ì´ë¸Œë¦¬ë“œ í”¼ì³ 160ì°¨ì›)")
        print("=" * 80)
        print(f"  (Val Set: NG={val_ng_count}ê°œ, Good={val_good_count}ê°œ)")
        print(f"  (ì„ íƒ(k): 15ê°œ)")
        print("-" * 80)
        
        # ëª¨ë“  ëª¨ë¸ ê²°ê³¼ ì¶œë ¥
        for model_name, result in models_results.items():
            print(f"  ğŸ“Š {model_name}:")
            print(f"    - Task 1 (ROC-AUC): {result['auc']:.4f}")
            print(f"    - Task 2 (Net Profit): {result['profit']:,.0f} ì›")
            print(f"    - ğŸ† ìµœì¢… ì ìˆ˜ (Total): {result['score']:.4f}")
            print("-" * 80)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê°•ì¡°
        print(f"ğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
        print(f"   ìµœê³  ì ìˆ˜: {best_result['score']:.4f}")
        print("=" * 80)
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ìˆœìœ„
        sorted_models = sorted(models_results.items(), key=lambda x: x[1]['score'], reverse=True)
        print("ğŸ“ˆ ì„±ëŠ¥ ìˆœìœ„:")
        for i, (model_name, result) in enumerate(sorted_models, 1):
            print(f"   {i}. {model_name}: {result['score']:.4f}")
        
        return models_results, best_model_name


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pipeline = HybridModelPipeline(n_epochs=20, batch_size=32) # ì—í¬í¬ì™€ ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ
    pipeline.run_comparison_pipeline()

if __name__ == "__main__":
    main()