import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
import warnings
import matplotlib.pyplot as plt
import platform

# 1. í•œê¸€ í°íŠ¸ ì„¤ì •
if platform.system() == 'Windows':
    plt.rcParams['font.family'] = 'Malgun Gothic'
elif platform.system() == 'Darwin':
    plt.rcParams['font.family'] = 'AppleGothic'
elif platform.system() == 'Linux':
    plt.rcParams['font.family'] = 'NanumGothic'

# 2. ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

# ----------------------------------------------------
# 1. ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤ (display.pyì™€ ë™ì¼)
# ----------------------------------------------------
class DataProcessor:
    """ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.OE = None
        self.Scaler = None
        self.cat_list = None
        self.num_list = None
        self.x_min_global = None
        self.x_max_global = None
        self.y_min_global = None
        self.y_max_global = None
        self.basic_feature_dim = None 
    
    def load_data(self, train_path="train.csv", test_path="test.csv"): 
        """ë°ì´í„° ë¡œë”©"""
        self.train = pd.read_csv(train_path)
        self.test = pd.read_csv(test_path)
        
        print(f"Train shape: {self.train.shape}")
        print(f"Test shape: {self.test.shape}") 
        
        self.train_X_basic = self.train.drop(columns=['Class']).iloc[:,:-256*3]
        self.train_Y = self.train['Class'].apply(lambda x: 1 if x == 'NG' else 0) # NG=1
        
        # ğŸ”¥ test ë°ì´í„° ì²˜ë¦¬ ì¶”ê°€
        self.test_X_basic = self.test.drop(columns=['ID']).iloc[:,:-256*3]
        
        print(f"Features shape (Train): {self.train_X_basic.shape}")
        print(f"Features shape (Test): {self.test_X_basic.shape}")
        print(f"Target distribution - Good: {(self.train_Y==0).sum()}, NG: {(self.train_Y==1).sum()}")
        
        return self.train, self.test, self.train_X_basic, self.train_Y, self.test_X_basic
    
    def setup_basic_preprocessing(self, train_X_basic_df):
        """ê¸°ë³¸ í”¼ì²˜ ì „ì²˜ë¦¬ ì„¤ì • (ì „ì²´ train setìœ¼ë¡œ fit)"""
        self.cat_list = train_X_basic_df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()
        self.num_list = sorted(list(set(train_X_basic_df.columns) - set(self.cat_list)))
        
        self.OE = OneHotEncoder(min_frequency=0.01, handle_unknown='infrequent_if_exist', sparse_output=False)
        self.OE.fit(train_X_basic_df[self.cat_list])
        
        self.Scaler = StandardScaler()
        self.Scaler.fit(train_X_basic_df[self.num_list])
        
    def preprocess_basic(self, dataset):
        """ê¸°ë³¸ í”¼ì²˜ ì „ì²˜ë¦¬"""
        Xc = self.OE.transform(dataset[self.cat_list])
        Xn = self.Scaler.transform(dataset[self.num_list])
        combined = np.concatenate([Xc, Xn], axis=1)
        
        if self.basic_feature_dim is None:
            self.basic_feature_dim = combined.shape[1]
            print(f"ê¸°ë³¸ í”¼ì²˜ ì°¨ì›: {self.basic_feature_dim}")
            
        return combined.astype(np.float32)
    
    def analyze_coordinate_range(self):
        """ì‹¤ì œ x, y ì¢Œí‘œ ë²”ìœ„ ë¶„ì„"""
        x_cols = [f'x{i}' for i in range(256)]
        y_cols = [f'y{i}' for i in range(256)]
        
        all_data = pd.concat([self.train, self.test], ignore_index=True) 
        x_values = all_data[x_cols].values.flatten()
        y_values = all_data[y_cols].values.flatten()
        
        x_values = x_values[~np.isnan(x_values)]
        y_values = y_values[~np.isnan(y_values)]
        
        self.x_min_global = x_values.min()
        self.x_max_global = x_values.max()
        self.y_min_global = y_values.min()
        self.y_max_global = y_values.max()
        
        print(f"ğŸ“Š ì¢Œí‘œ ë²”ìœ„ ë¶„ì„ ê²°ê³¼:")
        print(f"   X ì¢Œí‘œ ë²”ìœ„: {self.x_min_global:.2f} ~ {self.x_max_global:.2f}")
        print(f"   Y ì¢Œí‘œ ë²”ìœ„: {self.y_min_global:.2f} ~ {self.y_max_global:.2f}")
        
        return self.x_min_global, self.x_max_global, self.y_min_global, self.y_max_global

# ----------------------------------------------------
# 2. ë˜ìŠ¤í„°í™” í´ë˜ìŠ¤ (display.pyì™€ ë™ì¼)
# ----------------------------------------------------
class SpatialRasterizer:
    def __init__(self, x_min, x_max, y_min, y_max, grid_size=64):
        self.x_min = x_min
        self.x_max = x_max
        self.y_min = y_min
        self.y_max = y_max
        self.grid_size = grid_size
        self.x_range = x_max - x_min if x_max > x_min else 1
        self.y_range = y_max - y_min if y_max > y_min else 1
        
    def rasterize_with_real_coordinates(self, data_row):
        x_cols = [f'x{i}' for i in range(256)]
        y_cols = [f'y{i}' for i in range(256)]
        p_cols = [f'p{i}' for i in range(256)]
        
        x_coords = data_row[x_cols].values
        y_coords = data_row[y_cols].values
        p_values = data_row[p_cols].values
        
        grid = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)
        count_grid = np.zeros((self.grid_size, self.grid_size), dtype=np.int8)

        for i in range(256):
            if not (np.isnan(x_coords[i]) or np.isnan(y_coords[i]) or np.isnan(p_values[i])):
                x_norm = (x_coords[i] - self.x_min) / self.x_range
                y_norm = (y_coords[i] - self.y_min) / self.y_range
                x_idx = int(np.clip(x_norm * (self.grid_size - 1), 0, self.grid_size - 1))
                y_idx = int(np.clip(y_norm * (self.grid_size - 1), 0, self.grid_size - 1))
                grid[y_idx, x_idx] += p_values[i]
                count_grid[y_idx, x_idx] += 1
        
        mask = count_grid > 0
        grid[mask] = grid[mask] / count_grid[mask]
        
        return grid

# ----------------------------------------------------
# 3. E2E ëª¨ë¸ ì •ì˜ (display.pyì™€ ë™ì¼)
# ----------------------------------------------------

class ImageCNN(nn.Module):
    def __init__(self, output_dim=64, input_size=64):
        super(ImageCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) 
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)
        
        self.batch_norm1 = nn.BatchNorm2d(32)
        self.batch_norm2 = nn.BatchNorm2d(64)
        self.batch_norm3 = nn.BatchNorm2d(128)
        self.batch_norm4 = nn.BatchNorm2d(256)
        
        final_size = input_size // 16
        self.fc1 = nn.Linear(256 * final_size * final_size, 512)
        self.fc_out = nn.Linear(512, output_dim) 
        
        self.dropout = nn.Dropout(0.3)
    
    def forward(self, x):
        x = F.max_pool2d(F.relu(self.batch_norm1(self.conv1(x))), 2)
        x = F.max_pool2d(F.relu(self.batch_norm2(self.conv2(x))), 2)
        x = F.max_pool2d(F.relu(self.batch_norm3(self.conv3(x))), 2)
        x = F.max_pool2d(F.relu(self.batch_norm4(self.conv4(x))), 2)
        
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        return self.fc_out(x)


class FullE2EModel(nn.Module):
    def __init__(self, basic_feature_dim, image_cnn_output_dim=64, basic_mlp_output_dim=32, input_grid_size=64):
        super(FullE2EModel, self).__init__()
        
        self.image_cnn = ImageCNN(output_dim=image_cnn_output_dim, input_size=input_grid_size)
        
        self.basic_mlp = nn.Sequential(
            nn.Linear(basic_feature_dim, basic_feature_dim * 2),
            nn.ReLU(),
            nn.BatchNorm1d(basic_feature_dim * 2),
            nn.Dropout(0.3),
            nn.Linear(basic_feature_dim * 2, basic_mlp_output_dim),
            nn.ReLU()
        )
        
        combined_dim = image_cnn_output_dim + basic_mlp_output_dim
        self.head = nn.Sequential(
            nn.Linear(combined_dim, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(0.3),
            nn.Linear(64, 1) 
        )
    
    def forward(self, x_image, x_basic):
        img_feat = self.image_cnn(x_image)
        basic_feat = self.basic_mlp(x_basic)
        combined = torch.cat((img_feat, basic_feat), dim=1)
        output = self.head(combined) # 96ì°¨ì› -> 1ì°¨ì›
        return output

    # í”¼ì²˜ ì¶”ì¶œì„ ìœ„í•œ 'ë¨¸ë¦¬ ì—†ëŠ”' forward
    def extract_features(self, x_image, x_basic):
        img_feat = self.image_cnn(x_image)
        basic_feat = self.basic_mlp(x_basic)
        combined = torch.cat((img_feat, basic_feat), dim=1)
        return combined # 96ì°¨ì› í”¼ì²˜ ë°˜í™˜


# ----------------------------------------------------
# 4. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (display.pyì™€ ë™ì¼)
# ----------------------------------------------------
class MultiModalDataset(Dataset):
    def __init__(self, full_df, basic_features_np, rasterizer, labels_np=None):
        self.full_df = full_df.reset_index(drop=True)
        self.basic_features_np = basic_features_np
        self.rasterizer = rasterizer
        self.labels_np = labels_np
        self.is_test = (labels_np is None)

    def __len__(self):
        return len(self.full_df)

    def __getitem__(self, idx):
        data_row = self.full_df.iloc[idx]
        image_grid = self.rasterizer.rasterize_with_real_coordinates(data_row)
        image_tensor = torch.from_numpy(image_grid).unsqueeze(0) # (1, 64, 64)
        basic_feat_tensor = torch.from_numpy(self.basic_features_np[idx])
        
        if self.is_test:
            return image_tensor, basic_feat_tensor
        else:
            label_tensor = torch.tensor(self.labels_np[idx], dtype=torch.float32).view(1)
            return image_tensor, basic_feat_tensor, label_tensor

# ----------------------------------------------------
# 5. ğŸ”¥ ìµœì¢… ì œì¶œ íŒŒì´í”„ë¼ì¸ (train/validation ë¶„ë¦¬ ì—†ìŒ)
# ----------------------------------------------------
class ProductionPipeline:
    """í•˜ì´ë¸Œë¦¬ë“œ (CNN + RandomForest) ìµœì¢… ì œì¶œ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, n_epochs=13, batch_size=32):
        self.data_processor = DataProcessor()
        self.rasterizer = None
        self.cnn_model = None # E2E ëª¨ë¸
        self.rf_model = None  # RandomForest ëª¨ë¸
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        self.n_epochs = n_epochs
        self.batch_size = batch_size
    
    def train_cnn_extractor(self, train_loader):
        """E2E ëª¨ë¸ì„ 'í•™ìŠµ'ì‹œì¼œ í”¼ì²˜ ì¶”ì¶œê¸°ë¥¼ ë§Œë“­ë‹ˆë‹¤."""
        
        self.cnn_model = FullE2EModel(self.data_processor.basic_feature_dim).to(self.device)
        optimizer = optim.Adam(self.cnn_model.parameters(), lr=1e-3, weight_decay=1e-5)
        criterion = nn.BCEWithLogitsLoss() 
        
        print("\nğŸ§  CNN í”¼ì²˜ ì¶”ì¶œê¸° í•™ìŠµ ì‹œì‘...")
        
        for epoch in range(self.n_epochs):
            self.cnn_model.train()
            train_loss_total = 0.0
            
            for img, basic, labels in train_loader:
                img, basic, labels = img.to(self.device), basic.to(self.device), labels.to(self.device)
                optimizer.zero_grad()
                outputs = self.cnn_model(img, basic) # E2E ëª¨ë¸ í•™ìŠµ
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                train_loss_total += loss.item()
            
            avg_train_loss = train_loss_total / len(train_loader)
            print(f"  Epoch [{epoch+1}/{self.n_epochs}], Train Loss: {avg_train_loss:.4f}")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        torch.save(self.cnn_model.state_dict(), 'production_model.pth')
        print("âœ… CNN í”¼ì²˜ ì¶”ì¶œê¸° í•™ìŠµ ì™„ë£Œ ë° ì €ì¥.")

    def extract_cnn_features(self, loader, is_test=False):
        """'í•™ìŠµëœ' E2E ëª¨ë¸ì„ ì‚¬ìš©í•´ 'CNN í”¼ì²˜'ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        
        # ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
        if self.cnn_model is None:
            self.cnn_model = FullE2EModel(self.data_processor.basic_feature_dim).to(self.device)
        self.cnn_model.load_state_dict(torch.load('production_model.pth'))
        self.cnn_model.eval()
        
        all_features = []
        with torch.no_grad():
            if is_test:
                for img, basic in loader: # test ë°ì´í„°ëŠ” ë ˆì´ë¸” ì—†ìŒ
                    img, basic = img.to(self.device), basic.to(self.device)
                    features_batch = self.cnn_model.extract_features(img, basic)
                    all_features.append(features_batch.cpu().numpy())
            else:
                for img, basic, labels in loader: # train ë°ì´í„°ëŠ” ë ˆì´ë¸” ìˆìŒ
                    img, basic = img.to(self.device), basic.to(self.device)
                    features_batch = self.cnn_model.extract_features(img, basic)
                    all_features.append(features_batch.cpu().numpy())
        
        return np.concatenate(all_features, axis=0)

    def run_production_pipeline(self):
        """ìµœì¢… ì œì¶œìš© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=" * 60)
        print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ìµœì¢… ì œì¶œ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë”©
        print("\nğŸ“ 1ë‹¨ê³„: ë°ì´í„° ë¡œë”© (train.csv, test.csv)")
        train_df, test_df, train_X_basic_df, train_Y_series, test_X_basic_df = \
            self.data_processor.load_data(train_path="train.csv", test_path="test.csv")
        
        # 2. ì¢Œí‘œ ë²”ìœ„ ë¶„ì„
        print("\nğŸ“Š 2ë‹¨ê³„: ì¢Œí‘œ ë²”ìœ„ ë¶„ì„ (Train+Test í†µí•©)")
        x_min, x_max, y_min, y_max = self.data_processor.analyze_coordinate_range()
        
        # 3. ë˜ìŠ¤í„°í™” ì„¤ì •
        print("\nğŸ¯ 3ë‹¨ê³„: ê³µê°„ ë˜ìŠ¤í„°í™” ì„¤ì •")
        self.rasterizer = SpatialRasterizer(x_min, x_max, y_min, y_max, grid_size=64)
        
        # 4. ê¸°ë³¸ í”¼ì²˜ ì „ì²˜ë¦¬ (ì „ì²´ train ë°ì´í„°ë¡œ fit)
        print("\nğŸ”„ 4ë‹¨ê³„: ê¸°ë³¸ í”¼ì²˜ ì „ì²˜ë¦¬ (ì „ì²´ train ë°ì´í„°)")
        self.data_processor.setup_basic_preprocessing(train_X_basic_df)
        
        X_train_basic_np = self.data_processor.preprocess_basic(train_X_basic_df)
        X_test_basic_np = self.data_processor.preprocess_basic(test_X_basic_df)
        
        print(f"  ì „ì²˜ë¦¬ëœ ê¸°ë³¸ í”¼ì²˜ í˜•íƒœ (Train): {X_train_basic_np.shape}")
        print(f"  ì „ì²˜ë¦¬ëœ ê¸°ë³¸ í”¼ì²˜ í˜•íƒœ (Test): {X_test_basic_np.shape}")
        
        # 5. CNN í•™ìŠµìš© ë°ì´í„°ì…‹/ë¡œë” ìƒì„±
        print("\nğŸ“¦ 5ë‹¨ê³„: CNN í•™ìŠµìš© ë°ì´í„°ì…‹/ë¡œë” ìƒì„±")
        train_dataset = MultiModalDataset(train_df, X_train_basic_np, self.rasterizer, train_Y_series.values)
        test_dataset = MultiModalDataset(test_df, X_test_basic_np, self.rasterizer, labels_np=None) # testëŠ” ë ˆì´ë¸” ì—†ìŒ
        
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)
        
        # 6. CNN í”¼ì²˜ ì¶”ì¶œê¸° í•™ìŠµ
        print("\nğŸ§  6ë‹¨ê³„: CNN í”¼ì²˜ ì¶”ì¶œê¸° í•™ìŠµ")
        self.train_cnn_extractor(train_loader)
        
        # 7. CNN í”¼ì²˜ ì¶”ì¶œ (train, test ëª¨ë‘)
        print("\nâœ¨ 7ë‹¨ê³„: CNN í”¼ì²˜ ì¶”ì¶œ (Train/Test)")
        train_loader_seq = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=False)
        
        X_train_cnn_feats = self.extract_cnn_features(train_loader_seq, is_test=False)
        X_test_cnn_feats = self.extract_cnn_features(test_loader, is_test=True)
        
        print(f"  ì¶”ì¶œëœ CNN í”¼ì²˜ í˜•íƒœ (Train): {X_train_cnn_feats.shape}") 
        print(f"  ì¶”ì¶œëœ CNN í”¼ì²˜ í˜•íƒœ (Test): {X_test_cnn_feats.shape}")   
        
        # 8. í•˜ì´ë¸Œë¦¬ë“œ í”¼ì²˜ ìƒì„±
        print("\nğŸ§¬ 8ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ í”¼ì²˜ ê²°í•© (ê¸°ë³¸ + CNN)")
        X_train_hybrid = np.concatenate([X_train_basic_np, X_train_cnn_feats], axis=1)
        X_test_hybrid = np.concatenate([X_test_basic_np, X_test_cnn_feats], axis=1)
        
        print(f"  í•˜ì´ë¸Œë¦¬ë“œ í”¼ì²˜ í˜•íƒœ (Train): {X_train_hybrid.shape}")
        print(f"  í•˜ì´ë¸Œë¦¬ë“œ í”¼ì²˜ í˜•íƒœ (Test): {X_test_hybrid.shape}")
        
        # 9. RandomForest ëª¨ë¸ í•™ìŠµ
        print("\nğŸ¤– 9ë‹¨ê³„: RandomForest ëª¨ë¸ í•™ìŠµ (í•˜ì´ë¸Œë¦¬ë“œ í”¼ì²˜)")
        self.rf_model = RandomForestClassifier(random_state=42, n_estimators=200, n_jobs=-1)
        self.rf_model.fit(X_train_hybrid, train_Y_series)
        print("âœ… RandomForest ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")
        
        # 10. Test ë°ì´í„° ì˜ˆì¸¡
        print("\nğŸ”® 10ë‹¨ê³„: Test ë°ì´í„° ë¶ˆëŸ‰ë¥  ì˜ˆì¸¡")
        test_prob = self.rf_model.predict_proba(X_test_hybrid)[:, 1]  # NG(1)ì¼ í™•ë¥ 
        print(f"  ì˜ˆì¸¡ ì™„ë£Œ: {len(test_prob)}ê°œ ìƒ˜í”Œ")
        print(f"  ë¶ˆëŸ‰ë¥  ë²”ìœ„: {test_prob.min():.4f} ~ {test_prob.max():.4f}")
        
        # 11. ì œì¶œ íŒŒì¼ ìƒì„± (Sample_code.ipynb ì–‘ì‹)
        print("\nğŸ“ 11ë‹¨ê³„: ì œì¶œ íŒŒì¼ ìƒì„±")
        submission = pd.read_csv("sample_submission.csv")
        
        # ğŸ”¥ Sample_code.ipynbì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        # test ë°ì´í„°ì˜ ë¶ˆëŸ‰ë¥ ì„ L, P ê°ê°ì— ë³µì‚¬ (concatenate)
        submission['probability'] = np.concatenate([test_prob, test_prob])
        
        # ë¶ˆëŸ‰ë¥ ì´ ë‚®ì€ ìˆœì„œë¡œ Top 200ê°œì”© ì„ íƒ
        decision_id_L_list = submission.iloc[:466].sort_values('probability').iloc[:200]['ID']
        decision_id_P_list = submission.iloc[466:].sort_values('probability').iloc[:200]['ID']
        
        # decision=Trueë¡œ ì„¤ì •
        submission.loc[submission['ID'].isin(decision_id_L_list), 'decision'] = True
        submission.loc[submission['ID'].isin(decision_id_P_list), 'decision'] = True
        
        # ì œì¶œ íŒŒì¼ ì €ì¥
        submission.to_csv("hybrid_submission.csv", index=False)
        
        print("âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: hybrid_submission.csv")
        print(f"   - L íƒ€ì…ì—ì„œ ì„ íƒëœ ê°œìˆ˜: {len(decision_id_L_list)}")
        print(f"   - P íƒ€ì…ì—ì„œ ì„ íƒëœ ê°œìˆ˜: {len(decision_id_P_list)}")
        print(f"   - ì´ ì„ íƒëœ ê°œìˆ˜: {len(decision_id_L_list) + len(decision_id_P_list)}")
        
        # 12. ìµœì¢… ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("ğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ìµœì¢… ì œì¶œ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        print("=" * 60)
        print(f"  ğŸ”¹ ì‚¬ìš©ëœ í”¼ì²˜: ê¸°ë³¸ í”¼ì²˜ ({self.data_processor.basic_feature_dim}ì°¨ì›) + CNN í”¼ì²˜ (96ì°¨ì›)")
        print(f"  ğŸ”¹ ìµœì¢… í”¼ì²˜ ì°¨ì›: {X_train_hybrid.shape[1]}ì°¨ì›")
        print(f"  ğŸ”¹ í•™ìŠµ ë°ì´í„°: {len(train_Y_series)}ê°œ (NG: {train_Y_series.sum()}ê°œ, Good: {len(train_Y_series)-train_Y_series.sum()}ê°œ)")
        print(f"  ğŸ”¹ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_prob)}ê°œ")
        print(f"  ğŸ”¹ ì œì¶œ íŒŒì¼: hybrid_submission.csv")
        print("=" * 60)
        
        return submission


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pipeline = ProductionPipeline(n_epochs=13, batch_size=32)
    submission_result = pipeline.run_production_pipeline()
    
    # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
    print("\nğŸ“‹ ì œì¶œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°:")
    print(submission_result.head(10))
    print("...")
    print(submission_result.tail(10))
    
    # ì„ íƒëœ ê°œìˆ˜ í™•ì¸
    selected_count = submission_result['decision'].sum()
    print(f"\nâœ… ìµœì¢… ì„ íƒëœ ì œí’ˆ ê°œìˆ˜: {selected_count}ê°œ")

if __name__ == "__main__":
    main()
